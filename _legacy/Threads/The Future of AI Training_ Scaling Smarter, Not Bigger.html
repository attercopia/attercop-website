<!DOCTYPE html>
<!-- saved from url=(0083)https://threads.attercop.com/p/the-future-of-ai-training-scaling-smarter-not-bigger -->
<html lang="en" class="h-full antialiased"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link rel="preload" as="image" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/thumb_A_Beehiiv.png"><link rel="preload" as="image" href="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/publication/logo/6e1ab6f4-51af-49db-84aa-8499b10b14bd/A_Beehiiv.png"><link rel="preload" as="image" href="https://media.beehiiv.com/cdn-cgi/image/format=auto,width=800,height=421,fit=scale-down,onerror=redirect/uploads/asset/file/4d090dd4-b2c3-4a6d-b835-075831b3d0e2/thumb.png"><link rel="preload" as="image" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/thumbnail.png"><link rel="preload" as="image" href="https://media.beehiiv.com/cdn-cgi/image/format=auto,width=800,height=421,fit=scale-down,onerror=redirect/uploads/asset/file/482a00ce-718e-459e-bf7a-6941d7473562/Untitled_design__1_.jpg"><link rel="preload" as="image" href="https://media.beehiiv.com/cdn-cgi/image/format=auto,width=800,height=421,fit=scale-down,onerror=redirect/uploads/asset/file/7f3e4673-ff03-42dd-940d-b7b16552bdbd/t.jpg"><link rel="preload" as="image" href="https://media.beehiiv.com/cdn-cgi/image/format=auto,width=800,height=421,fit=scale-down,onerror=redirect/uploads/asset/file/0114b751-24c3-4580-8b68-182309778d2e/Ai_Agents_Thumbnail_sized.jpg"><link rel="preload" as="image" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/Untitled_design.png"><link rel="preload" as="image" href="https://media.beehiiv.com/cdn-cgi/image/format=auto,width=800,height=421,fit=scale-down,onerror=redirect/uploads/asset/file/a2a0f5e3-8c4c-4d73-aec8-a88b12cd82fe/Evolution_of_AI_Training1.jpg"><link rel="preload" as="image" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/Introductory_Post_Thumbnail.png"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="theme-color" content="#000000"><link rel="webmanifest" href="https://threads.attercop.com/manifest.webmanifest"><title>The Future of AI Training: Scaling Smarter, Not Bigger</title><meta name="keywords" content="artificial intelligence"><meta name="author" content="Attercop AI Threads"><meta property="og:type" content="website"><meta property="og:site_name" content="AI Threads"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta name="twitter:card" content="summary_large_image"><meta name="fb:app_id" content="1932054120322754"><link rel="icon" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/thumb_A_Beehiiv.png" as="image"><link rel="preload" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/css2" as="style"><link rel="preload" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/css2(1)" as="style"><link rel="alternate" type="application/rss+xml" title="AI Threads RSS Feed" href="https://rss.beehiiv.com/feeds/XTJ6gNWfGC.xml"><link rel="apple-touch-icon" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/thumb_A_Beehiiv.png"><link rel="apple-touch-startup-image" href="https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/publication/thumbnail/6e1ab6f4-51af-49db-84aa-8499b10b14bd/landscape_AI_Threads_no_Line.png"><link rel="preconnect" href="https://fonts.googleapis.com/"><link rel="preconnect" href="https://fonts.gstatic.com/"><script>window.__AppGlobals__={"SENTRY_CLIENT_DSN":"https://35c3cc890abe9dbb51e6e513fcd6bbca@o922922.ingest.us.sentry.io/4507170453979136","SENTRY_ENV":"production","SENTRY_RELEASE":"3cff6a1705cbdc8382a973baf6d2edb088993db4","STRIPE_PUBLISHABLE_KEY":"pk_live_51IekcQKHPFAlBzyyGNBguT5BEI7NEBqrTxJhsYN1FI1lQb9iWxU5U2OXfi744NEMx5p7EDXh08YXrudrZkkG9bGc00ZCrkXrxL","VAPID_PUBLIC_KEY":"BEhdtfPr1iefl9Jd16511ML4L5eC4dp4exGTAqE95rZEgjRPqc-k1FymD_b-e7XaC5g43hejZ0y_VGJq72zncjY","VITE_HUMAN_ENABLED":"true","VITE_HUMAN_URL":"//client.px-cloud.net/PXeBumDLwe/main.min.js","VITE_ADNETWORK_PIXELJS_URL":"https://beehiiv-adnetwork-production.s3.amazonaws.com/pixel-js.js","VITE_ADNETWORK_PIXELV2_URL":"https://beehiiv-adnetwork-production.s3.amazonaws.com/pixel-v2.js","RELEASE_VERSION":"v233","VITE_BIRDIE_CLIENT_ID":"q2je1lwj","ENABLE_SENTRY_DEV":false};</script><script>
window.addEventListener("beforeinstallprompt", (e) => {
  e.preventDefault();
  window.__DeferredPrompt__ = e;
});
          </script><link rel="stylesheet" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/css2"><link rel="stylesheet" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/css2(1)"><link rel="stylesheet" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/css2(1)"><link rel="stylesheet" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/css2(1)"><link rel="stylesheet" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/css2(1)"><link rel="stylesheet" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/css2(1)"><link rel="stylesheet" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/css2(1)"><link rel="stylesheet" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/css2(1)"><link rel="stylesheet" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/css2(1)"><script type="application/ld+json">{"@context":"https://schema.org","@type":"WebPage","identifier":"the-future-of-ai-training-scaling-smarter-not-bigger","url":"http://threads.attercop.com/p/the-future-of-ai-training-scaling-smarter-not-bigger","mainEntityOfPage":{"@type":"WebPage","@id":"http://threads.attercop.com/p/the-future-of-ai-training-scaling-smarter-not-bigger"},"headline":"The Future of AI Training: Scaling Smarter, Not Bigger","description":"Welcome to the first issue of AI Threads, where we take a closer look at how AI is built, trained, and scaled—and whether the way we’re doing it now is really the best approach.","datePublished":"2025-02-26T09:00:00.000Z","dateModified":"2025-03-27T09:45:25Z","isAccessibleForFree":true,"image":{"@type":"ImageObject","url":"https://beehiiv-images-production.s3.amazonaws.com/uploads/asset/file/a2a0f5e3-8c4c-4d73-aec8-a88b12cd82fe/Evolution_of_AI_Training1.jpg?t=1740402710"},"author":{"@type":"Person","name":"Attercop Team","image":{"@type":"ImageObject","contentUrl":"https://beehiiv-images-production.s3.amazonaws.com/static_assets/defaults/profile_picture.png","thumbnailUrl":"https://beehiiv-images-production.s3.amazonaws.com/static_assets/defaults/thumb_profile_picture.png"}},"publisher":{"@type":"Organization","name":"AI Threads","url":"https://threads.attercop.com/","identifier":"6e1ab6f4-51af-49db-84aa-8499b10b14bd","sameAs":["http://linkedin.com/company/attercop/"],"logo":{"@type":"ImageObject","url":"https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/publication/logo/6e1ab6f4-51af-49db-84aa-8499b10b14bd/A_Beehiiv.png","contentUrl":"https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/publication/logo/6e1ab6f4-51af-49db-84aa-8499b10b14bd/A_Beehiiv.png","thumbnailUrl":"https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/publication/logo/6e1ab6f4-51af-49db-84aa-8499b10b14bd/thumb_A_Beehiiv.png"},"image":{"@type":"ImageObject","url":"https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/publication/logo/6e1ab6f4-51af-49db-84aa-8499b10b14bd/A_Beehiiv.png","contentUrl":"https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/publication/logo/6e1ab6f4-51af-49db-84aa-8499b10b14bd/A_Beehiiv.png","thumbnailUrl":"https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/publication/logo/6e1ab6f4-51af-49db-84aa-8499b10b14bd/thumb_A_Beehiiv.png"}},"breadcrumb":{"@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"https://threads.attercop.com/","name":"AI Threads"}},{"@type":"ListItem","position":2,"item":{"@id":"http://threads.attercop.com/p/the-future-of-ai-training-scaling-smarter-not-bigger","name":"The Future of AI Training: Scaling Smarter, Not Bigger"}}]}}</script><style>:root {
  --wt-primary-color: #0B4251;
  --wt-text-on-primary-color: #FFFFFF;

  --wt-secondary-color: #91D3C9;
  --wt-text-on-secondary-color: #FFFFFF;

  --wt-tertiary-color: #FFFFFF;
  --wt-text-on-tertiary-color: #222222;

  --wt-background-color: #FFFFFF;
  --wt-text-on-background-color: #0B4251;

  --wt-subscribe-background-color: #FFFFFF;
  --wt-text-on-subscribe-background-color: #0B4251;

  --wt-header-font: "Poppins", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,"Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
  --wt-body-font: "Figtree", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
  --wt-button-font: "Poppins", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";

  --wt-border-radius: 6px
}

.bg-wt-primary { background-color: var(--wt-primary-color); }
.text-wt-primary { color: var(--wt-primary-color); }
.border-wt-primary { border-color: var(--wt-primary-color); }

.bg-wt-text-on-primary { background-color: var(--wt-text-on-primary-color); }
.text-wt-text-on-primary { color: var(--wt-text-on-primary-color); }
.border-wt-text-on-primary { border-color: var(--wt-text-on-primary-color); }

.bg-wt-secondary { background-color: var(--wt-secondary-color); }
.text-wt-secondary { color: var(--wt-secondary-color); }
.border-wt-secondary { border-color: var(--wt-secondary-color); }

.bg-wt-text-on-secondary { background-color: var(--wt-text-on-secondary-color); }
.text-wt-text-on-secondary { color: var(--wt-text-on-secondary-color); }
.border-wt-text-on-secondary { border-color: var(--wt-text-on-secondary-color); }

.bg-wt-tertiary { background-color: var(--wt-tertiary-color); }
.text-wt-tertiary { color: var(--wt-tertiary-color); }
.border-wt-tertiary { border-color: var(--wt-tertiary-color); }

.bg-wt-text-on-tertiary { background-color: var(--wt-text-on-tertiary-color); }
.text-wt-text-on-tertiary { color: var(--wt-text-on-tertiary-color); }
.border-wt-text-on-tertiary { border-color: var(--wt-text-on-tertiary-color); }

.bg-wt-background { background-color: var(--wt-background-color); }
.text-wt-background { color: var(--wt-background-color); }
.border-wt-background { border-color: var(--wt-background-color); }

.bg-wt-text-on-background { background-color: var(--wt-text-on-background-color); }
.text-wt-text-on-background { color: var(--wt-text-on-background-color); }
.border-wt-text-on-background { border-color: var(--wt-text-on-background-color); }

.bg-wt-subscribe-background { background-color: var(--wt-subscribe-background-color); }
.text-wt-subscribe-background { color: var(--wt-subscribe-background-color); }
.border-wt-subscribe-background { border-color: var(--wt-subscribe-background-color); }

.bg-wt-text-on-subscribe-background { background-color: var(--wt-text-on-subscribe-background-color); }
.text-wt-text-on-subscribe-background { color: var(--wt-text-on-subscribe-background-color); }
.border-wt-text-on-subscribe-background { border-color: var(--wt-text-on-subscribe-background-color); }

.rounded-wt { border-radius: var(--wt-border-radius); }

.wt-header-font { font-family: var(--wt-header-font); }
.wt-body-font { font-family: var(--wt-body-font); }
.wt-button-font { font-family: var(--wt-button-font); }

input:focus { --tw-ring-color: transparent !important; }

li a { word-break: break-word; }

@media only screen and (max-width:667px) {
  .mob-stack {
    display: block !important;
    width: 100% !important;
  }

  .mob-w-full {
    width: 100% !important;
  }
}

</style><link rel="stylesheet" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/root-BShfP9hX.css"><style id="_goober"> @keyframes go2264125279{from{transform:scale(0) rotate(45deg);opacity:0;}to{transform:scale(1) rotate(45deg);opacity:1;}}@keyframes go3020080000{from{transform:scale(0);opacity:0;}to{transform:scale(1);opacity:1;}}@keyframes go463499852{from{transform:scale(0) rotate(90deg);opacity:0;}to{transform:scale(1) rotate(90deg);opacity:1;}}@keyframes go1268368563{from{transform:rotate(0deg);}to{transform:rotate(360deg);}}@keyframes go1310225428{from{transform:scale(0) rotate(45deg);opacity:0;}to{transform:scale(1) rotate(45deg);opacity:1;}}@keyframes go651618207{0%{height:0;width:0;opacity:0;}40%{height:0;width:6px;opacity:1;}100%{opacity:1;height:10px;}}@keyframes go901347462{from{transform:scale(0.6);opacity:0.4;}to{transform:scale(1);opacity:1;}}.go4109123758{z-index:9999;}.go4109123758 > *{pointer-events:auto;}</style><script src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/main.min.js" async=""></script><script src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/main.d0fa628558f415ecb9ce.js" crossorigin="anonymous"></script><link href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/main2.css" rel="stylesheet"><meta name="description" content="AI’s rapid growth comes with challenges. Are massive GPU clusters the future, or is there a smarter way to train AI? Explore the alternatives."><meta property="og:url" content="https://threads.attercop.com/p/the-future-of-ai-training-scaling-smarter-not-bigger"><meta property="og:title" content="The Future of AI Training: Scaling Smarter, Not Bigger"><meta property="og:description" content="Bigger AI models need bigger infrastructure, but are we reaching the limits of centralised training? Discover the challenges and new approaches."><meta property="og:image" content="https://beehiiv-images-production.s3.amazonaws.com/uploads/asset/file/a2a0f5e3-8c4c-4d73-aec8-a88b12cd82fe/Evolution_of_AI_Training1.jpg?t=1740402710"><meta property="og:image:alt" content="Welcome to the first issue of AI Threads, where we take a closer look at how AI is built, trained, and scaled—and whether the way we’re doing it now is really the best approach."><meta name="twitter:url" content="https://threads.attercop.com/p/the-future-of-ai-training-scaling-smarter-not-bigger"><meta name="twitter:title" content="The Future of AI Training: Scaling Smarter, Not Bigger"><meta name="twitter:description" content="Welcome to the first issue of AI Threads, where we take a closer look at how AI is built, trained, and scaled—and whether the way we’re doing it now is really the best approach."><meta name="twitter:image" content="https://beehiiv-images-production.s3.amazonaws.com/uploads/asset/file/a2a0f5e3-8c4c-4d73-aec8-a88b12cd82fe/Evolution_of_AI_Training1.jpg?t=1740402710"><link rel="canonical" href="https://threads.attercop.com/p/the-future-of-ai-training-scaling-smarter-not-bigger"><link rel="stylesheet" href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/BackButton-icJ-rWaN.css"></head><body class="flex h-full flex-col bg-wt-background text-wt-text-on-background" data-new-gr-c-s-check-loaded="14.1264.0" data-gr-ext-installed=""><div class=""><div class="flex min-h-screen flex-col"><div></div><div id="_rht_toaster" style="position:fixed;z-index:9999;top:16px;left:16px;right:16px;bottom:16px;pointer-events:none"></div><div class="w-full"><nav class="px-4 sm:px-6 py-2" style="background-color:#FFFFFF"><div class="mx-auto w-full max-w-6xl"><div class="mx-auto flex justify-between"><a class="rounded-md transition:all px-2 py-1 hover:bg-black/5" data-discover="true" href="https://threads.attercop.com/"><div class="flex items-center space-x-2"><div class="w-8 h-8 overflow-hidden rounded-wt"><figure class="aspect-square relative h-full overflow-hidden w-full"><img class="absolute inset-0 h-full w-full object-cover" width="100" height="100" src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/thumb_A_Beehiiv.png" alt="AI Threads logo"></figure></div><span style="color:#0B4251;font-family:Figtree, sans-ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto,&quot;Helvetica Neue&quot;, Arial, &quot;Noto Sans&quot;, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;" class="hidden text-sm sm:text-md wt-text-primary md:block text-md font-regular font-ariel">AI Threads</span></div></a><div class="flex items-center space-x-2"><div class="flex items-center space-x-2"><button class="border font-medium shadow-sm wt-button-font inline-flex items-center disabled:bg-gray-400 disabled:cursor-not-allowed focus:outline-none focus:ring-2 focus:ring-offset-2 justify-center transition-colors rounded-wt px-4 py-2 text-sm border-gray-300 text-gray-700 bg-white shadow-md focus:ring-wt-primary !text-sm !font-semibold shadow-none" style="background:#91D3C9;border:1px solid #FFFFFF;color:#FFFFFF;font-family:Poppins">Login</button><a class="border font-medium shadow-sm wt-button-font inline-flex items-center disabled:bg-gray-400 disabled:cursor-not-allowed focus:outline-none focus:ring-2 focus:ring-offset-2 justify-center transition-colors rounded-wt px-4 py-2 text-sm border-wt-primary text-wt-text-on-primary bg-wt-primary disabled:border-gray-500 focus:ring-wt-primary whitespace-nowrap !text-sm !font-semibold shadow-none" style="background:#0B4251;border:1px solid #0B4251;color:#FFFFFF;font-family:Poppins" data-discover="true" href="https://threads.attercop.com/subscribe">Subscribe</a><div class="relative inline-block text-left" data-headlessui-state=""><button aria-label="Menu" class="rounded-full transition-all" id="headlessui-menu-button-_R_3mbl5_" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><div class="mt-1.5 rounded hover:bg-black/5" style="color:#0B4251"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" data-slot="icon" class="h-6 w-6"><path fill-rule="evenodd" d="M5.25 9a6.75 6.75 0 0 1 13.5 0v.75c0 2.123.8 4.057 2.118 5.52a.75.75 0 0 1-.297 1.206c-1.544.57-3.16.99-4.831 1.243a3.75 3.75 0 1 1-7.48 0 24.585 24.585 0 0 1-4.831-1.244.75.75 0 0 1-.298-1.205A8.217 8.217 0 0 0 5.25 9.75V9Zm4.502 8.9a2.25 2.25 0 1 0 4.496 0 25.057 25.057 0 0 1-4.496 0Z" clip-rule="evenodd"></path></svg></div></button></div></div><div class="relative inline-block text-left" data-headlessui-state=""><button aria-label="Menu" class="rounded-full transition-all" id="headlessui-menu-button-_R_qbl5_" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><div class="mt-1.5 rounded hover:bg-black/5" style="color:#0B4251"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><div class="h-[1px] w-full opacity-5" style="background-color:#0B4251"></div><nav class="px-4 sm:px-6 py-2" style="background-color:#FFFFFF"><div class="mx-auto w-full max-w-6xl"><div class="mx-auto flex justify-center"><div class="block"><div class="z-20 flex gap-1"></div></div></div></div></nav></div><main class="flex-grow"><div><div class="sticky top-0 z-50 w-full"><div aria-valuemax="100" aria-valuemin="0" aria-valuenow="0" aria-valuetext="0%" role="progressbar" data-state="loading" data-value="0" data-max="100" class="relative overflow-hidden h-1.5 bg-transparent w-full"><div data-state="loading" data-value="0" data-max="100" class="h-full w-full bg-wt-primary" style="width: 0%;"></div></div></div><div class="fixed bottom-0 left-0 z-50 px-4"></div><script type="module">
var e, t, r, i, n;
(t = {}),
  (r = {}),
  null ==
    (i = (e =
      "undefined" != typeof globalThis
        ? globalThis
        : "undefined" != typeof self
          ? self
          : "undefined" != typeof window
            ? window
            : "undefined" != typeof global
              ? global
              : {}).parcelRequire326a) &&
    (((i = function (e) {
      if (e in t) return t[e].exports;
      if (e in r) {
        var i = r[e];
        delete r[e];
        var n = { id: e, exports: {} };
        return (t[e] = n), i.call(n.exports, n, n.exports), n.exports;
      }
      var o = Error("Cannot find module '" + e + "'");
      throw ((o.code = "MODULE_NOT_FOUND"), o);
    }).register = function (e, t) {
      r[e] = t;
    }),
    (e.parcelRequire326a = i)),
  (n = i.register)("gUOGs", function (e, t) {
    Object.defineProperty(e.exports, "__esModule", { value: !0 });
    var r,
      n = i("kBCj4"),
      o = i("53LBo"),
      s = [],
      a = (function () {
        function e(e) {
          (this.$$observationTargets = []),
            (this.$$activeTargets = []),
            (this.$$skippedTargets = []);
          var t =
            void 0 === e
              ? "Failed to construct 'ResizeObserver': 1 argument required, but only 0 present."
              : "function" != typeof e
                ? "Failed to construct 'ResizeObserver': The callback provided as parameter 1 is not a function."
                : void 0;
          if (t) throw TypeError(t);
          this.$$callback = e;
        }
        return (
          (e.prototype.observe = function (e) {
            var t = d("observe", e);
            if (t) throw TypeError(t);
            u(this.$$observationTargets, e) >= 0 ||
              (this.$$observationTargets.push(new n.ResizeObservation(e)),
              0 > s.indexOf(this) && (s.push(this), v()));
          }),
          (e.prototype.unobserve = function (e) {
            var t = d("unobserve", e);
            if (t) throw TypeError(t);
            var r = u(this.$$observationTargets, e);
            r < 0 ||
              (this.$$observationTargets.splice(r, 1),
              0 === this.$$observationTargets.length && c(this));
          }),
          (e.prototype.disconnect = function () {
            (this.$$observationTargets = []),
              (this.$$activeTargets = []),
              c(this);
          }),
          e
        );
      })();
    function c(e) {
      var t = s.indexOf(e);
      t >= 0 && (s.splice(t, 1), $());
    }
    function d(e, t) {
      return void 0 === t
        ? "Failed to execute '" +
            e +
            "' on 'ResizeObserver': 1 argument required, but only 0 present."
        : t && t.nodeType === window.Node.ELEMENT_NODE
          ? void 0
          : "Failed to execute '" +
            e +
            "' on 'ResizeObserver': parameter 1 is not of type 'Element'.";
    }
    function u(e, t) {
      for (var r = 0; r < e.length; r += 1) if (e[r].target === t) return r;
      return -1;
    }
    e.exports.ResizeObserver = a;
    var l = function (e) {
        s.forEach(function (t) {
          (t.$$activeTargets = []),
            (t.$$skippedTargets = []),
            t.$$observationTargets.forEach(function (r) {
              r.isActive() &&
                (h(r.target) > e
                  ? t.$$activeTargets.push(r)
                  : t.$$skippedTargets.push(r));
            });
        });
      },
      f = function () {
        var e = 1 / 0;
        return (
          s.forEach(function (t) {
            if (t.$$activeTargets.length) {
              var r = [];
              t.$$activeTargets.forEach(function (t) {
                var i = new o.ResizeObserverEntry(t.target);
                r.push(i),
                  (t.$$broadcastWidth = i.contentRect.width),
                  (t.$$broadcastHeight = i.contentRect.height);
                var n = h(t.target);
                n < e && (e = n);
              }),
                t.$$callback(r, t),
                (t.$$activeTargets = []);
            }
          }),
          e
        );
      },
      p = function () {
        var e = new window.ErrorEvent("ResizeLoopError", {
          message:
            "ResizeObserver loop completed with undelivered notifications.",
        });
        window.dispatchEvent(e);
      },
      h = function (e) {
        for (var t = 0; e.parentNode; ) (e = e.parentNode), (t += 1);
        return t;
      },
      g = function () {
        for (
          l(0);
          s.some(function (e) {
            return !!e.$$activeTargets.length;
          });

        )
          l(f());
        s.some(function (e) {
          return !!e.$$skippedTargets.length;
        }) && p();
      },
      v = function () {
        r || b();
      },
      b = function () {
        r = window.requestAnimationFrame(function () {
          g(), b();
        });
      },
      $ = function () {
        r &&
          !s.some(function (e) {
            return !!e.$$observationTargets.length;
          }) &&
          (window.cancelAnimationFrame(r), (r = void 0));
      };
    e.exports.install = function () {
      return (window.ResizeObserver = a);
    };
  }),
  n("kBCj4", function (e, t) {
    Object.defineProperty(e.exports, "__esModule", { value: !0 });
    var r = i("ardMU"),
      n = (function () {
        function e(e) {
          (this.target = e),
            (this.$$broadcastWidth = this.$$broadcastHeight = 0);
        }
        return (
          Object.defineProperty(e.prototype, "broadcastWidth", {
            get: function () {
              return this.$$broadcastWidth;
            },
            enumerable: !0,
            configurable: !0,
          }),
          Object.defineProperty(e.prototype, "broadcastHeight", {
            get: function () {
              return this.$$broadcastHeight;
            },
            enumerable: !0,
            configurable: !0,
          }),
          (e.prototype.isActive = function () {
            var e = r.ContentRect(this.target);
            return (
              !!e &&
              (e.width !== this.broadcastWidth ||
                e.height !== this.broadcastHeight)
            );
          }),
          e
        );
      })();
    e.exports.ResizeObservation = n;
  }),
  n("ardMU", function (e, t) {
    Object.defineProperty(e.exports, "__esModule", { value: !0 }),
      (e.exports.ContentRect = function (e) {
        if ("getBBox" in e) {
          var t = e.getBBox();
          return Object.freeze({
            height: t.height,
            left: 0,
            top: 0,
            width: t.width,
          });
        }
        var r = window.getComputedStyle(e);
        return Object.freeze({
          height: parseFloat(r.height || "0"),
          left: parseFloat(r.paddingLeft || "0"),
          top: parseFloat(r.paddingTop || "0"),
          width: parseFloat(r.width || "0"),
        });
      });
  }),
  n("53LBo", function (e, t) {
    Object.defineProperty(e.exports, "__esModule", { value: !0 });
    var r = i("ardMU");
    e.exports.ResizeObserverEntry = function (e) {
      (this.target = e), (this.contentRect = r.ContentRect(e));
    };
  }),
  (function () {
    if (
      !(function () {
        if (!window.__stwts || !Object.keys(window.__stwts).length)
          return (
            (window.__stwts = { buildVersion: "1.0.0", widgets: { init: !0 } }),
            !0
          );
      })()
    )
      return;
    function e(e, t) {
      t <= 300 && (e.style.width = "300px"),
        t >= 600 && (e.style.width = "600px"),
        t > 300 && t < 600 && (e.style.width = `${t}px`);
    }
    "undefined" == typeof ResizeObserver && i("gUOGs").install();
    let t = new (ResizeObserver || window.ResizeObserver)(function (t) {
      t.forEach((t) => {
        let r = t?.contentRect?.width,
          i = t.target.querySelector("iframe");
        r && i && e(i, r);
      });
    });

    function loadStocktwitsEmbeds() {
      Array.from(
        document.querySelectorAll("blockquote.stocktwits-embedded-post"),
      ).forEach((r) => {
        !(function (t, r) {
          if (t.getAttribute("visited")) return;
          let i = t.getAttribute("data-origin"),
            n = t.getAttribute("data-id"),
            o = document.createElement("div");
          (o.style.display = "flex"),
            (o.style.maxWidth = "600px"),
            (o.style.width = "100%"),
            (o.style.marginTop = "10px"),
            (o.style.marginBottom = "10px");
          let s = document.createElement("iframe");
          o.appendChild(s);
          let a = `${i}/embeddable/message/${n}`;
          (s.style.width = "640px"),
            (s.style.height = "0"),
            (s.style.maxWidth = "600px"),
            (s.style.position = "static"),
            (s.style.visibility = "visible"),
            (s.style.display = "block"),
            (s.style.flexGrow = "1"),
            s.setAttribute("frameborder", "0"),
            s.setAttribute("allowtransparency", "true"),
            s.setAttribute("scrolling", "no"),
            s.setAttribute("allowfullscreen", "true");
          let c = !1,
            d = null;
          function u(e) {
            c = !0;
            let { height: r } = e.data;
            void 0 !== r && (s.style.height = `${r}px`),
              document.body.contains(t) && t.remove();
          }
          t.parentNode.insertBefore(o, t), t.setAttribute("visited", true),
            s.addEventListener("load", function () {
              e(s, o.clientWidth),
                (function () {
                  let e = () => {
                    ((d = new MessageChannel()).port1.onmessage = u),
                      s.contentWindow.postMessage(
                        { message: "initialize" },
                        "*",
                        [d.port2],
                      );
                  };
                  e();
                  let t = setInterval(() => {
                    c ? clearInterval(t) : e();
                  }, 60);
                })();
            }),
            s.setAttribute("src", a),
            r.observe(o);
        })(r, t);
      });
    }

    window.__stwts.loadStocktwitsEmbeds = loadStocktwitsEmbeds;
  })();
</script><div class="relative mx-auto max-w-6xl px-4"><div class="fixed bottom-0 left-0 top-auto z-20 w-full rounded bg-wt-background shadow-xl transition-all duration-300 ease-in-out md:bottom-auto md:z-auto md:!w-fit md:border-none md:shadow-none opacity-100 md:top-36"><div class="absolute left-0 top-0 w-full border border-t border-wt-text-on-background bg-wt-background opacity-10 md:hidden"></div><div class="mx-auto w-full max-w-6xl px-0 lg:px-4"><div class="flex flex-col gap-8 md:h-40"><div class=""><div class="grid grid-cols-3 p-4 px-8 sm:p-2 sm:px-2 md:grid-cols-1 md:gap-2"><div class="relative flex flex-col justify-center md:left-1"><div class="flex h-7 w-7 items-center justify-center rounded-wt transition-all hover:bg-black/5"><button class="cursor-pointer"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-6 w-6 stroke-1 text-wt-text-on-background opacity-50"><path stroke-linecap="round" stroke-linejoin="round" d="M10.5 19.5 3 12m0 0 7.5-7.5M3 12h18"></path></svg></button></div></div><div class="flex items-center justify-center gap-3 md:flex-col md:items-start md:gap-1"><button class="group" type="button"><div class="text-wt-text-on-background opacity-50 group-hover:opacity-100 flex items-center"><div class="rounded-full p-1 hover:bg-[#f3f4f6]"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="h-7 w-7 outline-none h-4 w-4 stroke-1"><path stroke-linecap="round" stroke-linejoin="round" d="M21 8.25c0-2.485-2.099-4.5-4.688-4.5-1.935 0-3.597 1.126-4.312 2.733-.715-1.607-2.377-2.733-4.313-2.733C5.1 3.75 3 5.765 3 8.25c0 7.22 9 12 9 12s9-4.78 9-12Z"></path></svg></div><span class="hidden text-transparent text-sm font-medium group-hover:opacity-100">0</span></div></button><button type="button" class="group relative top-[1px] flex items-center outline-none md:pt-0"><div class="rounded-full p-1 hover:bg-[#f3f4f6]"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="h-7 w-7 stroke-1 text-wt-text-on-background opacity-50 group-hover:opacity-100"><path stroke-linecap="round" stroke-linejoin="round" d="M20.25 8.511c.884.284 1.5 1.128 1.5 2.097v4.286c0 1.136-.847 2.1-1.98 2.193-.34.027-.68.052-1.02.072v3.091l-3-3c-1.354 0-2.694-.055-4.02-.163a2.115 2.115 0 0 1-.825-.242m9.345-8.334a2.126 2.126 0 0 0-.476-.095 48.64 48.64 0 0 0-8.048 0c-1.131.094-1.976 1.057-1.976 2.192v4.286c0 .837.46 1.58 1.155 1.951m9.345-8.334V6.637c0-1.621-1.152-3.026-2.76-3.235A48.455 48.455 0 0 0 11.25 3c-2.115 0-4.198.137-6.24.402-1.608.209-2.76 1.614-2.76 3.235v6.226c0 1.621 1.152 3.026 2.76 3.235.577.075 1.157.14 1.74.194V21l4.155-4.155"></path></svg></div><span class="hidden relative -top-[1px] text-sm font-medium group-hover:opacity-100 md:top-0">0</span></button><div class="relative z-20 inline-block text-left md:z-0" data-headlessui-state=""><button class="relative right-[2px] top-[1.5px] outline-none md:right-[0px]" id="headlessui-menu-button-_r_1c_" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state=""><div class="flex items-center rounded-full p-1 hover:bg-[#f3f4f6]"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="text-wt-text-on-background opacity-50 hover:opacity-100 h-7 w-7 stroke-1 text-gray-400"><path stroke-linecap="round" stroke-linejoin="round" d="M7.217 10.907a2.25 2.25 0 1 0 0 2.186m0-2.186c.18.324.283.696.283 1.093s-.103.77-.283 1.093m0-2.186 9.566-5.314m-9.566 7.5 9.566 5.314m0 0a2.25 2.25 0 1 0 3.935 2.186 2.25 2.25 0 0 0-3.935-2.186Zm0-12.814a2.25 2.25 0 1 0 3.933-2.185 2.25 2.25 0 0 0-3.933 2.185Z"></path></svg></div></button></div></div></div></div></div></div></div><div class=""><div class="mx-auto flex max-w-2xl flex-col pb-4"><div class="mt-8" style="padding-left: 15px; padding-right: 15px;"><ul class="flex flex-wrap items-center gap-2 text-xs font-semibold"><li class="flex items-center gap-2 text-wt-text-on-background"><a href="https://threads.attercop.com/" class="opacity-70">AI Threads</a><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" height="14px"><path stroke-linecap="round" stroke-linejoin="round" d="m8.25 4.5 7.5 7.5-7.5 7.5"></path></svg></li><li class="flex items-center gap-2 text-wt-text-on-background"><span class="opacity-70">Posts</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" height="14px"><path stroke-linecap="round" stroke-linejoin="round" d="m8.25 4.5 7.5 7.5-7.5 7.5"></path></svg></li><li class="flex items-center gap-2 text-wt-text-on-background"><span class="!opacity-100">The Future of AI Training: Scaling Smarter, Not Bigger</span></li></ul></div><div><div><div><link href="https://fonts.gstatic.com/" rel="preconnect"><link href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/css2" rel="stylesheet"><link href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/css2(1)" rel="stylesheet"><link href="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/css2" rel="stylesheet"><style type="text/css">
:root {
  --wt-primary-color: #0B4251;
  --wt-text-on-primary-color: #FFFFFF;

  --wt-secondary-color: #91D3C9;
  --wt-text-on-secondary-color: #FFFFFF;

  --wt-tertiary-color: #FFFFFF;
  --wt-text-on-tertiary-color: #222222;

  --wt-background-color: #FFFFFF;
  --wt-text-on-background-color: #0B4251;

  --wt-subscribe-background-color: #FFFFFF;
  --wt-text-on-subscribe-background-color: #0B4251;

  --wt-header-font: "Poppins", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto,"Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
  --wt-body-font: "Figtree", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
  --wt-button-font: "Poppins", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";

  --wt-border-radius: 6px
}

.bg-wt-primary { background-color: var(--wt-primary-color); }
.text-wt-primary { color: var(--wt-primary-color); }
.border-wt-primary { border-color: var(--wt-primary-color); }

.bg-wt-text-on-primary { background-color: var(--wt-text-on-primary-color); }
.text-wt-text-on-primary { color: var(--wt-text-on-primary-color); }
.border-wt-text-on-primary { border-color: var(--wt-text-on-primary-color); }

.bg-wt-secondary { background-color: var(--wt-secondary-color); }
.text-wt-secondary { color: var(--wt-secondary-color); }
.border-wt-secondary { border-color: var(--wt-secondary-color); }

.bg-wt-text-on-secondary { background-color: var(--wt-text-on-secondary-color); }
.text-wt-text-on-secondary { color: var(--wt-text-on-secondary-color); }
.border-wt-text-on-secondary { border-color: var(--wt-text-on-secondary-color); }

.bg-wt-tertiary { background-color: var(--wt-tertiary-color); }
.text-wt-tertiary { color: var(--wt-tertiary-color); }
.border-wt-tertiary { border-color: var(--wt-tertiary-color); }

.bg-wt-text-on-tertiary { background-color: var(--wt-text-on-tertiary-color); }
.text-wt-text-on-tertiary { color: var(--wt-text-on-tertiary-color); }
.border-wt-text-on-tertiary { border-color: var(--wt-text-on-tertiary-color); }

.bg-wt-background { background-color: var(--wt-background-color); }
.text-wt-background { color: var(--wt-background-color); }
.border-wt-background { border-color: var(--wt-background-color); }

.bg-wt-text-on-background { background-color: var(--wt-text-on-background-color); }
.text-wt-text-on-background { color: var(--wt-text-on-background-color); }
.border-wt-text-on-background { border-color: var(--wt-text-on-background-color); }

.bg-wt-subscribe-background { background-color: var(--wt-subscribe-background-color); }
.text-wt-subscribe-background { color: var(--wt-subscribe-background-color); }
.border-wt-subscribe-background { border-color: var(--wt-subscribe-background-color); }

.bg-wt-text-on-subscribe-background { background-color: var(--wt-text-on-subscribe-background-color); }
.text-wt-text-on-subscribe-background { color: var(--wt-text-on-subscribe-background-color); }
.border-wt-text-on-subscribe-background { border-color: var(--wt-text-on-subscribe-background-color); }

.rounded-wt { border-radius: var(--wt-border-radius); }

.wt-header-font { font-family: var(--wt-header-font); }
.wt-body-font { font-family: var(--wt-body-font); }
.wt-button-font { font-family: var(--wt-button-font); }

input:focus { --tw-ring-color: transparent !important; }

li a { word-break: break-word; }

@media only screen and (max-width:667px) {
  .mob-stack {
    display: block !important;
    width: 100% !important;
  }

  .mob-w-full {
    width: 100% !important;
  }
}

</style><style>
        @font-face {
          font-family: 'Poppins';
          font-style: normal;
          font-weight: 400;
          font-display: swap;
          src: url('https://fonts.gstatic.com/s/poppins/v22/pxiEyp8kv8JHgFVrJJfecnFHGPc.woff2') format('woff2');
        }

        @font-face {
          font-family: 'Poppins';
          font-style: italic;
          font-weight: 400;
          font-display: swap;
          src: url('https://fonts.gstatic.com/s/poppins/v22/pxiGyp8kv8JHgFVrJJLucHtAOvWDSA.woff2') format('woff2');
        }
        @font-face {
          font-family: 'Poppins';
          font-style: normal;
          font-weight: 700;
          font-display: swap;
          src: url('https://fonts.gstatic.com/s/poppins/v22/pxiByp8kv8JHgFVrLCz7Z1xlFd2JQEk.woff2') format('woff2');
        }

        @font-face {
          font-family: 'Poppins';
          font-style: italic;
          font-weight: 700;
          font-display: swap;
          src: url('https://fonts.gstatic.com/s/poppins/v22/pxiDyp8kv8JHgFVrJJLmy15VF9eOYktMqg.woff2') format('woff2');
        }
</style><script async="" id="tiktok-script" src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/embed.js" type="text/javascript"></script><script async="" defer="" id="twitter-wjs" src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/widgets.js" type="text/javascript"></script><style>
  .table-base, .table-c, .table-h { border: 1px solid #0b4251; }
  .table-c { padding:5px; background-color:#FFFFFF; }
  .table-c p { color: #222222; font-family:'Poppins',Helvetica,sans-serif !important; overflow-wrap: break-word; }
  .table-h { padding:5px; background-color:#bad3d0; }
  .table-h p { color: #222222; font-family:'Poppins',Helvetica,sans-serif !important; overflow-wrap: break-word; }
</style></div><div class="bg-wt-background" style="color: var(--wt-text-on-background-color) !important;"><div class="rendered-post" style="max-width: 672px; margin: 0 auto;"><div class="bg-wt-background text-wt-text-on-background"><div style="padding-top:1.5rem; padding-bottom:1.5rem;"><style>
  .bh__byline_wrapper {
    font-size: .875rem;
    line-height: 1.25rem;
    vertical-align: middle;
    justify-content: space-between;
    display: block;
  }
  
  .bh__byline_social_wrapper {
    display: flex;
    margin-top: 0.5rem;
    align-items: center;
  }
  
  .bh__byline_social_wrapper > * + * {
    margin-left: 0.5rem;
  }
  
  @media (min-width: 768px) {
    .bh__byline_wrapper {
      display: flex;
    }
  
    .bh__byline_social_wrapper {
      margin-top: 0rem;
    }
  }
</style><div id="web-header" style="padding-left: 15px; padding-right: 15px; color: var(--wt-text-on-background-color) !important; padding-bottom: 2rem;"><h1 style="font-size: 36px; font-family:&#39;Poppins&#39;,Helvetica,sans-serif; line-height: 2.5rem; padding-bottom: 8px; font-weight: 700;">The Future of AI Training: Scaling Smarter, Not Bigger</h1><h2 style="font-size: 20px; font-family:&#39;Poppins&#39;,Helvetica,sans-serif; line-height: 1.75rem; padding-bottom: 20px; font-weight: 400;">Welcome to the first issue of AI Threads, where we take a closer look at how AI is built, trained, and scaled—and whether the way we’re doing it now is really the best approach.</h2><div class="bh__byline_wrapper"><div><div style="display:flex;"><div style="display:flex; flex-direction:row; justify-content:flex-start;"><div style="display:flex;"><img alt="Author" height="35" src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/thumb_profile_picture.png" style="height:35px;width:35px;border:3px solid rgba(0,0,0,0);left:-3px;z-index:1;box-sizing:content-box;position:relative;border-radius:9999px;" width="35"></div><div style="display:flex; align-items:center;"><p style="left:6px;position:relative;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-weight:normal;font-size:14px;color: var(--wt-text-on-background-color);margin:0px;line-height:20px;"><span>Attercop Team</span><br><span class="text-wt-text-on-background" style="opacity:0.75;"> February 26, 2025 </span></p></div></div></div></div><div class="bh__byline_social_wrapper"><a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fthreads.attercop.com%2Fp%2Fthe-future-of-ai-training-scaling-smarter-not-bigger" target="_blank"><div style="max-width:28px;"><svg fill="none" height="100%" viewBox="0 0 212 212" width="100%" xmlns="http://www.w3.org/2000/svg"><circle cx="106.214" cy="105.5" fill-opacity="0.15" fill="#9CA3AF" r="105.5"></circle><path d="M96.9223 92.1778H87.0327V105.498H96.9223V145.461H113.405V105.498H125.273L126.591 92.1778H113.405V86.5165C113.405 83.5193 114.064 82.1873 117.031 82.1873H126.591V65.5364H114.064C102.197 65.5364 96.9223 70.8647 96.9223 80.8552V92.1778Z" fill="#BAC2CE"></path></svg></div></a><a href="https://twitter.com/intent/tweet?text=Welcome+to+the+first+issue+of+AI+Threads%2C+where+we+take+a+closer+look+at+how+AI+is+built%2C+trained%2C+and+scaled%E2%80%94and+whether+the+way+we%E2%80%99re+doing+it+now+is+really+the+best+approach.&amp;url=https%3A%2F%2Fthreads.attercop.com%2Fp%2Fthe-future-of-ai-training-scaling-smarter-not-bigger" target="_blank"><div style="max-width:28px;"><svg fill="none" height="100%" viewBox="0 0 52 52" width="100%" xmlns="http://www.w3.org/2000/svg"><circle cx="26" cy="26" fill-opacity="0.15" fill="#9CA3AF" r="26"></circle><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z" fill="#BAC2CE" transform="translate(14,14)"></path></svg></div></a><a href="https://www.threads.net/intent/post?text=Welcome+to+the+first+issue+of+AI+Threads%2C+where+we+take+a+closer+look+at+how+AI+is+built%2C+trained%2C+and+scaled%E2%80%94and+whether+the+way+we%E2%80%99re+doing+it+now+is+really+the+best+approach.+https%3A%2F%2Fthreads.attercop.com%2Fp%2Fthe-future-of-ai-training-scaling-smarter-not-bigger" target="_blank"><div style="max-width:28px;"><svg fill="none" height="100%" viewBox="0 0 211 211" width="100%" xmlns="http://www.w3.org/2000/svg"><circle cx="105.5" cy="105.5" fill-opacity="0.15" fill="#9CA3AF" r="105.5"></circle><path d="M125.185 102.469C124.828 102.298 124.465 102.133 124.097 101.975C123.456 90.1702 117.006 83.4121 106.175 83.3429C106.126 83.3426 106.077 83.3426 106.028 83.3426C99.5502 83.3426 94.1624 86.1078 90.8463 91.1396L96.8028 95.2256C99.2801 91.4671 103.168 90.6658 106.031 90.6658C106.064 90.6658 106.097 90.6658 106.13 90.6661C109.696 90.6889 112.387 91.7257 114.129 93.7477C115.397 95.2198 116.244 97.254 116.664 99.8213C113.502 99.2839 110.083 99.1187 106.427 99.3283C96.13 99.9214 89.5101 105.927 89.9547 114.272C90.1803 118.505 92.2891 122.147 95.8924 124.526C98.9389 126.537 102.863 127.52 106.941 127.297C112.326 127.002 116.551 124.948 119.498 121.19C121.737 118.337 123.152 114.64 123.777 109.981C126.344 111.53 128.246 113.568 129.296 116.019C131.083 120.184 131.187 127.028 125.602 132.608C120.709 137.496 114.827 139.611 105.938 139.677C96.0779 139.603 88.6207 136.441 83.7723 130.278C79.2321 124.506 76.8857 116.17 76.7982 105.5C76.8857 94.8301 79.2321 86.4937 83.7723 80.7222C88.6207 74.5587 96.0778 71.3965 105.938 71.3232C115.87 71.3971 123.457 74.5745 128.491 80.7677C130.959 83.8048 132.82 87.6242 134.047 92.0775L141.028 90.2151C139.54 84.7337 137.2 80.0102 134.016 76.0929C127.563 68.1529 118.124 64.0844 105.962 64H105.914C93.777 64.0841 84.4441 68.1681 78.1742 76.1384C72.5949 83.2311 69.7169 93.1 69.6202 105.471L69.6199 105.5L69.6202 105.529C69.7169 117.9 72.5949 127.769 78.1742 134.862C84.4441 142.832 93.777 146.916 105.914 147H105.962C116.753 146.925 124.358 144.1 130.624 137.84C138.822 129.65 138.575 119.385 135.873 113.083C133.934 108.564 130.239 104.893 125.185 102.469ZM106.555 119.985C102.042 120.239 97.3533 118.213 97.1221 113.874C96.9507 110.657 99.4116 107.067 106.832 106.64C107.682 106.591 108.516 106.567 109.335 106.567C112.03 106.567 114.552 106.829 116.844 107.33C115.989 118.008 110.974 119.742 106.555 119.985Z" fill="#BAC2CE"></path></svg></div></a><a href="https://www.linkedin.com/sharing/share-offsite?url=https%3A%2F%2Fthreads.attercop.com%2Fp%2Fthe-future-of-ai-training-scaling-smarter-not-bigger" target="_blank"><div style="max-width:28px;"><svg fill="none" height="100%" viewBox="0 0 211 211" width="100%" xmlns="http://www.w3.org/2000/svg"><circle cx="105.5" cy="105.5" fill-opacity="0.15" fill="#9CA3AF" r="105.5"></circle><path d="M82.1892 75.4698C82.1892 80.1362 78.526 83.8026 73.8638 83.8026C69.2015 83.8026 65.5383 80.1362 65.5383 75.4698C65.5383 70.8034 69.2015 67.137 73.8638 67.137C78.526 67.137 82.1892 70.8034 82.1892 75.4698ZM82.1892 90.4689H65.5383V143.799H82.1892V90.4689ZM108.831 90.4689H92.1797V143.799H108.831V115.801C108.831 100.135 128.812 98.8017 128.812 115.801V143.799H145.463V110.134C145.463 83.8026 115.824 84.8026 108.831 97.8018V90.4689Z" fill="#BAC2CE"></path></svg></div></a></div></div></div><div style="padding-bottom:2rem;"><img style="margin-left:auto; margin-right:auto;" src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/Evolution_of_AI_Training1.jpg"></div><div id="content-blocks"><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> AI is in the middle of a major infrastructure boom. In the race to build more powerful models, companies are pouring billions into computing power, pushing the limits of what massive data centres can handle. The thinking has been simple:&nbsp;<b>scale is the answer.</b>&nbsp;More GPUs, bigger clusters, faster training. But as these systems grow, so do the problems—higher costs, technical bottlenecks, and diminishing returns. </p></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> That’s where things get interesting. In this issue of&nbsp;AI Threads, we’ll be diving into the race for computing power, the limits of centralised AI training, and whether a more distributed approach could offer a better way forward. </p></div><div style="padding:30px;"><div style="margin: 0 auto; border-top: 3px solid #91d3c9; width:50%;"></div></div><div id="the-cost-of-winning-a-is-billion-do" style="padding-bottom:4px;padding-left:15px;padding-right:15px;padding-top:16px;"><h3 style="color:#0b4251;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:20px;font-weight:normal;line-height:30px;margin:0;text-align:left;">The Cost of Winning: AI’s Billion-Dollar Bet</h3></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> Microsoft’s recent announcement offers a glimpse into just how high the stakes have become.&nbsp;In a&nbsp;<a class="link" href="https://blogs.microsoft.com/on-the-issues/2025/01/03/the-golden-opportunity-for-american-ai?utm_source=threads.attercop.com&amp;utm_medium=referral&amp;utm_campaign=the-future-of-ai-training-scaling-smarter-not-bigger" target="_blank" style="-webkit-text-decoration:underline #ec554b;color:#ec554b;font-style:italic;text-decoration:underline #ec554b;word-break:break-word;;">blog post</a>&nbsp;earlier this year, the company revealed—almost in passing—that it plans to invest&nbsp;<b>$80 billion</b>&nbsp;in AI-enabled data centres in 2025 alone. If that number feels abstract, consider this: the&nbsp;<b>James Webb Space Telescope</b>&nbsp;cost a “mere”&nbsp;<b>$10 billion</b>. In other words, Microsoft is committing the equivalent of&nbsp;<b>eight James Webbs—in a single year—just to train and deploy AI models.</b></p></div><div style="padding-bottom:12px;padding-left:19px;padding-right:19px;padding-top:12px;"><div style="background-color:#FFFFFF;padding-bottom:7px;padding-left:12px;padding-right:12px;padding-top:7px;border-left-color:#ec554b;border-left-style:solid;border-left-width:2px;border-radius:0px;"><div style="left"><p style="color:#222222;font-family:Poppins;font-size:15px;line-height:Normal;margin:0;padding-bottom:12px;padding-top:12px;text-align:justify;"><span style="color:#0B4251;"><i>“Artificial intelligence is the electricity of our age, and the next four years can build a foundation for America’s economic success for the next quarter century.”</i></span></p></div><div style="padding-top:10;text-align:left;"><small style="color:#222222;font-size:12px;"><span style="color:#0B4251;"><b>Brad Smith, Microsoft Vice Chair &amp; President</b></span></small></div></div></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> This somewhat mind-boggling comparison underscores the extraordinary scale—and cost—of the new race in artificial intelligence. The winners in this race are those who can secure the&nbsp;<b>largest, most powerful compute clusters</b>. Once, the world’s richest moguls competed over yachts, jets, and private islands. Now, bragging rights belong to those who can build the&nbsp;<b>mightiest GPU farm</b>, with Elon Musk and Mark Zuckerberg showcasing plans for data centers housing&nbsp;<b>hundreds of thousands of GPUs</b>. More recently, even&nbsp;<b>Trump has weighed in</b>, teaming up with OpenAI, Oracle, and SoftBank to announce the&nbsp;<a class="link" href="https://openai.com/index/announcing-the-stargate-project/?utm_source=threads.attercop.com&amp;utm_medium=referral&amp;utm_campaign=the-future-of-ai-training-scaling-smarter-not-bigger" target="_blank" style="-webkit-text-decoration:underline #ec554b;color:#ec554b;font-style:italic;text-decoration:underline #ec554b;word-break:break-word;;"><b>$500 billion Stargate initiative</b></a><b>.</b></p></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> Yet, this arms race for ever-increasing GPU scale faces a fundamental problem—one that goes beyond the&nbsp;<b>moral and practical concerns of its massive energy consumption</b>. Every time you add another chip, you multiply the complexity of keeping them synchronised. A significant portion of training time is lost to&nbsp;<b>“checkpointing”</b>—the necessary process of exchanging updates after each training step. Could the solution lie in distributing AI training across&nbsp;<b>multiple smaller data centers—or even a diffuse network of ordinary smartphones—rather than relying on a single massive cluster?</b>Let’s take a closer look. </p></div><div style="padding:30px;"><div style="margin: 0 auto; border-top: 3px solid #91d3c9; width:50%;"></div></div><div id="a-new-arms-race-in-ai" style="padding-bottom:4px;padding-left:15px;padding-right:15px;padding-top:16px;"><h3 style="color:#0b4251;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:20px;font-weight:normal;line-height:30px;margin:0;text-align:justify;"><b>A New Arms Race in AI</b></h3></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> OpenAI’s GPT-4 was famously trained on&nbsp;<b>approximately 25,000 GPUs</b>&nbsp;less than two years ago. Now, Elon Musk claims he has&nbsp;<b>100,000 chips</b>&nbsp;in one data centre and wants to buy&nbsp;<b>200,000 more</b>. Mark Zuckerberg says he’s aiming for&nbsp;<b>350,000 GPUs</b>&nbsp;at Meta. And, as we’ve just seen, Microsoft is already&nbsp;plotting&nbsp;to spend&nbsp;<b>$80 billion</b>&nbsp;in a single year to ramp up its AI operations. All this investment highlights how massive the computations behind modern&nbsp;<b>Large Language Models (LLMs)</b>&nbsp;have become. </p></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> The rationale for scaling up is clear:&nbsp;more GPUs can, in theory, complete training runs faster by sharing the workload. However, the key word is&nbsp;<i>theory</i>. In practice,&nbsp;parallelising&nbsp;the work requires constant communication between the chips to keep them synchronised with the latest updates. This communication overhead can&nbsp;balloon, creating a bottleneck known as&nbsp;<b>diminishing returns</b>. </p></div><div style="padding:30px;"><div style="margin: 0 auto; border-top: 3px solid #91d3c9; width:50%;"></div></div><div id="parallel-processing-a-double-edged-" style="padding-bottom:4px;padding-left:15px;padding-right:15px;padding-top:16px;"><h3 style="color:#0b4251;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:20px;font-weight:normal;line-height:30px;margin:0;text-align:justify;"><b>Parallel Processing: A Double-Edged Sword</b></h3></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> Modern AI systems learn through&nbsp;<i>backpropagation</i>. You show the model vast amounts of data—perhaps text with certain words blanked out or partially hidden images—and it guesses the missing bits. If it guesses incorrectly, you tweak its many parameters so that next time, it edges closer to the right answer. </p></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> This process is repeated&nbsp;<b>millions or even billions of times</b>, which is why training large models is so computationally intensive. The speed boost from adding extra GPUs is significant at first, but once you reach&nbsp;<b>tens of thousands of GPUs</b>, more and more time is lost shuttling information around&nbsp;<b>rather than on the learning itself</b>. </p></div><div style="padding:30px;"><div style="margin: 0 auto; border-top: 3px solid #91d3c9; width:50%;"></div></div><div id="di-lo-co-a-smarter-way-to-distribut" style="padding-bottom:4px;padding-left:15px;padding-right:15px;padding-top:16px;"><h3 style="color:#0b4251;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:20px;font-weight:normal;line-height:30px;margin:0;text-align:justify;"><b>DiLoCo: A Smarter Way to Distribute Training</b></h3></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> Arthur Douillard at Google DeepMind noticed something crucial:&nbsp;<b>do you really need to synchronise all GPUs all the time?</b>&nbsp;In a&nbsp;<a class="link" href="https://arxiv.org/abs/2311.08105?utm_source=threads.attercop.com&amp;utm_medium=referral&amp;utm_campaign=the-future-of-ai-training-scaling-smarter-not-bigger" target="_blank" style="-webkit-text-decoration:underline #ec554b;color:#ec554b;font-style:italic;text-decoration:underline #ec554b;word-break:break-word;;">2023 paper on “Distributed Low-Communication Training of Language Models” (DiLoCo)</a>, Douillard introduced the idea of grouping GPUs into&nbsp;<b>“islands”</b>—essentially&nbsp;<b>data centres</b>—where they communicate as usual <i>within</i> each island but synchronise less frequently <i>between</i> islands. </p></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> This simple tweak&nbsp;<b>massively reduces</b>&nbsp;the communication overhead. There&nbsp;is&nbsp;a cost in raw performance when measuring the model’s accuracy on the specific task it was trained on, but interestingly, there can be improvements in&nbsp;<i>generalised</i> performance—the ability to handle completely different tasks or questions. It’s a bit like having separate groups of students who study on their own, explore tangents, and then occasionally come together to share what they’ve learned. The end result can be&nbsp;<b>more well-rounded expertise</b>. </p></div><div style="padding-left:15px;padding-right:15px;"><div style="padding-bottom:20px;padding-left:0px;padding-right:0px;padding-top:20px;"><img alt="" style="margin:0 auto 0 auto;width:100%;" src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/Screenshot_2025-02-19_at_17.07.50.png"><div style="margin:0 auto 0 auto;padding-left:0px;padding-right:0px;padding-top:4px;text-align:center;width:100%;"><small style="color:#000000;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:12px;font-style:italic;text-decoration-color:#000000;text-decoration:none;"><p>A visual representation of the DiLoCo training process, where multiple AI model replicas are trained independently across different compute clusters before periodic synchronisation.<br><a class="link" href="https://arxiv.org/abs/2311.08105?utm_source=threads.attercop.com&amp;utm_medium=referral&amp;utm_campaign=the-future-of-ai-training-scaling-smarter-not-bigger" target="_blank" style="-webkit-text-decoration:underline #ec554b;color:#ec554b;font-style:italic;text-decoration:underline #ec554b;word-break:break-word;;">Source: Google DeepMind, from&nbsp;Distributed Low-Communication Training of Language Models</a></p></small></div></div></div><div style="padding:30px;"><div style="margin: 0 auto; border-top: 3px solid #91d3c9; width:50%;"></div></div><div id="prime-intellects-approach-open-di-l" style="padding-bottom:4px;padding-left:15px;padding-right:15px;padding-top:16px;"><h3 style="color:#0b4251;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:20px;font-weight:normal;line-height:30px;margin:0;text-align:justify;"><b>Prime Intellect’s Approach: OpenDiLoCo</b></h3></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> This idea has already been put into practice. Vincent Weisser, founder of Prime Intellect, led the training of an LLM called “<a class="link" href="https://www.primeintellect.ai/blog/intellect-1?utm_source=threads.attercop.com&amp;utm_medium=referral&amp;utm_campaign=the-future-of-ai-training-scaling-smarter-not-bigger" target="_blank" style="-webkit-text-decoration:underline #ec554b;color:#ec554b;font-style:italic;text-decoration:underline #ec554b;word-break:break-word;;">Intellect-1</a>” in November 2024. Comparable in size to Meta’s Llama 2, Intellect-1&nbsp;<b>wasn’t trained on a single giant cluster </b>but rather on&nbsp;<i>30</i>&nbsp;smaller ones, scattered across eight cities on three continents. </p></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> Their system,&nbsp;<a class="link" href="https://arxiv.org/abs/2407.07852?utm_source=threads.attercop.com&amp;utm_medium=referral&amp;utm_campaign=the-future-of-ai-training-scaling-smarter-not-bigger" target="_blank" style="-webkit-text-decoration:underline #ec554b;color:#ec554b;font-style:italic;text-decoration:underline #ec554b;word-break:break-word;;">OpenDiLoCo</a>,&nbsp;<b>only checkpoints once every 500 steps</b>. It also&nbsp;<b>“quantises” the changes</b> - discarding the least important chunks of information—so that each cluster spends more time computing and less time communicating. Even with clusters so far apart, Weisser’s team kept the GPUs&nbsp;<b>“actively working” 83% of the time</b>. Restricting the data centres to North America&nbsp;<b>increased that figure to 96%</b>. </p></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> The best part? Smaller data centres are far more widely available and affordable. As a result, Weisser’s open-source lab can train a&nbsp;<b>multi-billion-parameter</b>&nbsp;model without having to rent (or build) a colossal, monolithic GPU farm. </p></div><div style="padding:30px;"><div style="margin: 0 auto; border-top: 3px solid #91d3c9; width:50%;"></div></div><div id="the-privacy-perk-federated-and-spli" style="padding-bottom:4px;padding-left:15px;padding-right:15px;padding-top:16px;"><h3 style="color:#0b4251;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:20px;font-weight:normal;line-height:30px;margin:0;text-align:justify;"><b>The Privacy Perk: Federated and Split Learning</b></h3></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> While&nbsp;cost and scale&nbsp;are major motivators for distributed training, there’s another benefit that might be even more significant:&nbsp;<i>privacy</i>. In fields like healthcare, patient records are both invaluable for AI research and extremely sensitive. Conventional training approaches, which&nbsp;<b>combine all raw data in a central location</b>, are often unworkable. That’s where techniques such as&nbsp;<b><a class="link" href="https://research.ibm.com/blog/what-is-federated-learning?utm_source=threads.attercop.com&amp;utm_medium=referral&amp;utm_campaign=the-future-of-ai-training-scaling-smarter-not-bigger" target="_blank" style="-webkit-text-decoration:underline #ec554b;color:#ec554b;font-style:italic;text-decoration:underline #ec554b;word-break:break-word;;">Federated Learning</a></b>&nbsp;(<b>FL</b>) and&nbsp;<b><a class="link" href="https://arxiv.org/pdf/2004.12088?utm_source=threads.attercop.com&amp;utm_medium=referral&amp;utm_campaign=the-future-of-ai-training-scaling-smarter-not-bigger" target="_blank" style="-webkit-text-decoration:underline #ec554b;color:#ec554b;font-style:italic;text-decoration:underline #ec554b;word-break:break-word;;">Split Learning</a></b>&nbsp;come into play. </p></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> In essence,&nbsp;<b>Federated Learning</b>&nbsp;trains a complete model locally on each participant’s machine or server, sending back only updated&nbsp;<i>parameters</i>—not the underlying raw data—to a central server for aggregation.&nbsp;<b>For example, hospitals could collaborate to build a cancer detection system without sharing actual scans or medical histories.</b></p></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> In&nbsp;<b>Split Learning</b>, the&nbsp;<b>neural network itself is divided into segments</b>, with some running on a central server and others on local machines, ensuring that sensitive data remains on-premises. </p></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> Together, these approaches help ensure that large-scale AI collaboration can proceed even when data is private or highly regulated. The concept of&nbsp;<i><a class="link" href="https://neurips.cc/virtual/2024/poster/96602?utm_source=threads.attercop.com&amp;utm_medium=referral&amp;utm_campaign=the-future-of-ai-training-scaling-smarter-not-bigger" target="_blank" style="-webkit-text-decoration:underline #ec554b;color:#ec554b;font-style:italic;text-decoration:underline #ec554b;word-break:break-word;;">Federated Behavioural Planes</a></i>, for instance, offers a way to analyse how different participants behave in a federated learning scenario,&nbsp;<b>detecting anomalies or malicious contributions</b>&nbsp;along the way. </p></div><div style="padding:30px;"><div style="margin: 0 auto; border-top: 3px solid #91d3c9; width:50%;"></div></div><div id="human-sensing-and-fl-protecting-pri" style="padding-bottom:4px;padding-left:15px;padding-right:15px;padding-top:16px;"><h3 style="color:#0b4251;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:20px;font-weight:normal;line-height:30px;margin:0;text-align:justify;"><b>Human Sensing and FL: Protecting Privacy</b></h3></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> As an example of&nbsp;<b>how</b>&nbsp;these techniques can improve privacy, let’s look at the emerging field of&nbsp;<b>Human Sensing</b>, where technology monitors human activities or physiological states. The sheer&nbsp;amount&nbsp;and intimacy of this data&nbsp;raise&nbsp;serious <b>ethical and legal concerns</b>. An interesting&nbsp;<a class="link" href="https://arxiv.org/abs/2501.04000?utm_source=threads.attercop.com&amp;utm_medium=referral&amp;utm_campaign=the-future-of-ai-training-scaling-smarter-not-bigger" target="_blank" style="-webkit-text-decoration:underline #ec554b;color:#ec554b;font-style:italic;text-decoration:underline #ec554b;word-break:break-word;;">2025 survey</a> explored&nbsp;how&nbsp;<b>Federated Learning</b>&nbsp;might allow researchers to advance this field without storing personal data in&nbsp;a&nbsp;central repository—an approach that could be applicable to&nbsp;a wide range of&nbsp;privacy requirements. </p></div><div style="padding-left:15px;padding-right:15px;"><div style="padding-bottom:20px;padding-left:0px;padding-right:0px;padding-top:20px;"><img alt="" style="margin:0 auto 0 auto;width:100%;" src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/Screenshot_2025-02-24_at_13.04.21.png"><div style="margin:0 auto 0 auto;padding-left:0px;padding-right:0px;padding-top:4px;text-align:center;width:100%;"><small style="color:#000000;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:12px;font-style:italic;text-decoration-color:#000000;text-decoration:none;"><p>A comparison of centralised learning, where all raw data is sent to a central server for AI model training, versus federated learning, where models are trained locally on devices without sharing raw data.<br><a class="link" href="https://arxiv.org/abs/2501.04000?utm_source=threads.attercop.com&amp;utm_medium=referral&amp;utm_campaign=the-future-of-ai-training-scaling-smarter-not-bigger" target="_blank" style="-webkit-text-decoration:underline #ec554b;color:#ec554b;font-style:italic;text-decoration:underline #ec554b;word-break:break-word;;">Source: Research from&nbsp;A Survey on Federated Learning in Human Sensing</a></p></small></div></div></div><div style="padding:30px;"><div style="margin: 0 auto; border-top: 3px solid #91d3c9; width:50%;"></div></div><div id="beyond-gp-us-could-your-smartphone-" style="padding-bottom:4px;padding-left:15px;padding-right:15px;padding-top:16px;"><h3 style="color:#0b4251;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:20px;font-weight:normal;line-height:30px;margin:0;text-align:justify;"><b>Beyond GPUs: Could Your Smartphone Join the Party?</b></h3></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> If&nbsp;distributing&nbsp;training across multiple data centres&nbsp;sounds&nbsp;radical, an even bolder vision&nbsp;involves&nbsp;tapping into billions of ordinary consumer devices. A single top-of-the-line&nbsp;Nvidia GPU&nbsp;equates to a few hundred premium smartphones in raw computational power—but if we collectively harness the world’s phones and laptops, <b>we&nbsp;could&nbsp;have far more&nbsp;processing power&nbsp;than any single data centre.</b></p></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> Of course, that dream&nbsp;comes with&nbsp;many challenges: network variability, data storage limitations, and the need to keep track of a myriad devices going offline. Yet, the same logic that allows&nbsp;<b>DiLoCo’s island-based approach to generalise well</b>&nbsp;might also apply to a smartphone-based system. Each device contributes slightly different data or perspectives, helping the model&nbsp;become more robust&nbsp;as a result. </p></div><div style="padding:30px;"><div style="margin: 0 auto; border-top: 3px solid #91d3c9; width:50%;"></div></div><div id="what-lies-ahead" style="padding-bottom:4px;padding-left:15px;padding-right:15px;padding-top:16px;"><h3 style="color:#0b4251;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:20px;font-weight:normal;line-height:30px;margin:0;text-align:justify;"><b>What Lies Ahead</b></h3></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> For the moment, sticking with a&nbsp;<b>monolithic data-centre</b>&nbsp;approach and&nbsp;<b>high-powered initiatives</b>&nbsp;like&nbsp;Stargate&nbsp;remains the&nbsp;most well-known, effective, and practical&nbsp;strategy—provided there is access to near-unlimited funding, of course!&nbsp;For this reason, the&nbsp;big players, including&nbsp;OpenAI, Microsoft, Google, and Meta, who have already poured massive resources into&nbsp;<b>single-site GPU clusters</b>, continue to invest&nbsp;billions&nbsp;into&nbsp;data centres&nbsp;and&nbsp;silicon. But the advantages of&nbsp;<b>distributed training</b>, with&nbsp;its&nbsp;<b>lower cost barriers, better resource availability, and strong privacy protections</b>, are becoming clearer by the day. </p></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"> DiLoCo, Prime Intellect’s&nbsp;OpenDiLoCo implementation, the various&nbsp;Federated and Split Learning solutions emerging, and&nbsp;growing research in this field&nbsp;all demonstrate that AI&nbsp;<i>can</i>&nbsp;be&nbsp;scaled out&nbsp;in ways that are both&nbsp;efficient and&nbsp;more accessible. And with&nbsp;smaller data centres&nbsp;becoming widely available, we might hope that the field shifts towards a future where&nbsp;<b>intensive AI training</b>&nbsp;isn’t reserved&nbsp;only&nbsp;for&nbsp;trillion-dollar corporations. </p></div><div style="padding-bottom:12px;padding-left:19px;padding-right:19px;padding-top:12px;"><div style="background-color:#FFFFFF;padding-bottom:7px;padding-left:12px;padding-right:12px;padding-top:7px;border-left-color:#ec554b;border-left-style:solid;border-left-width:2px;border-radius:0px;"><div style="left"><p style="color:#222222;font-family:Poppins;font-size:15px;line-height:Normal;margin:0;padding-bottom:12px;padding-top:12px;text-align:justify;"><span style="color:#0B4251;"><i>“OpenDiLoCo achieves 90-95% compute utilisation in real-world decentralised training across two continents and three countries.”</i></span></p></div><div style="padding-top:10;text-align:left;"><small style="color:#222222;font-size:12px;"><span style="color:rgb(0, 0, 0);font-family:-webkit-standard;font-size:medium;">-&nbsp;</span><span style="color:rgb(11, 66, 81);"><b>OpenDiLoCo Research Paper</b></span></small></div></div></div><div style="padding:30px;"><div style="margin: 0 auto; border-top: 3px solid #91d3c9; width:50%;"></div></div><div id="conclusion-democratising-the-future" style="padding-bottom:4px;padding-left:15px;padding-right:15px;padding-top:16px;"><h3 style="color:#0b4251;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:20px;font-weight:normal;line-height:30px;margin:0;text-align:justify;"><b>Conclusion: Democratising the Future of AI</b></h3></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:justify;"><span style="color:rgb(0, 0, 0);">While the GPU arms race continues, there are early signs that AI training may be evolving into something more nuanced.</span> As&nbsp;<b>‘AI optimists’,</b>&nbsp;we are prepared to hope that AI’s future won’t belong solely to those who can afford the&nbsp;biggest single cluster&nbsp;but to those who can coordinate clusters&nbsp;<i>intelligently</i>, wherever they happen to be. We might not see the dismantling of&nbsp;mega data centres&nbsp;in the short term, but the underlying trends—<b>distribution, collaboration, and privacy awareness</b>—show that there are promising ways to rewrite the rules of this&nbsp;incredibly expensive game. </p></div><div style="padding-left:15px;padding-right:15px;"><div style="padding-bottom:20px;padding-left:0px;padding-right:0px;padding-top:20px;"><img alt="" style="margin:0 auto 0 auto;width:100%;" src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/Screenshot_2025-02-24_at_14.44.15.png"><div style="margin:0 auto 0 auto;padding-left:0px;padding-right:0px;padding-top:4px;text-align:center;width:100%;"><small style="color:#000000;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:12px;font-style:italic;text-decoration-color:#000000;text-decoration:none;"><p>Global data centre distribution as shown on Data Center Map, highlighting the infrastructure supporting AI and cloud computing.<br><a class="link" href="https://www.datacentermap.com/datacenters/?utm_source=threads.attercop.com&amp;utm_medium=referral&amp;utm_campaign=the-future-of-ai-training-scaling-smarter-not-bigger" target="_blank" style="-webkit-text-decoration:underline #ec554b;color:#ec554b;font-style:italic;text-decoration:underline #ec554b;word-break:break-word;;">Source: Data Center Map.</a></p></small></div></div></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:left;"> It’s hard to argue with&nbsp;<b>$80 billion invested in a single year</b>. But that same figure also raises questions about the&nbsp;<b>sustainability, equity, and ethics</b>&nbsp;of concentrating so much computing power in so few hands. By spreading out AI training across&nbsp;<b>smaller clusters—or even thousands of personal devices</b>—we can keep pushing the boundaries of what’s possible while also&nbsp;<b>reducing costs, preserving data privacy, and ensuring that AI remains an asset for all, not just the privileged few</b>. </p></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:left;"></p></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:left;"> At the same time, advances in distributed training hold promise for lowering the&nbsp;astronomical costs&nbsp;of AI research. By harnessing multiple smaller clusters—rather than a&nbsp;single, colossally expensive data centre—organisations can&nbsp;<b>allocate budgets more efficiently</b>, enabling smaller nations (e.g., the UK!) to compete more effectively, while also&nbsp;<b>empowering academic and open-source projects</b>&nbsp;that lack the capital of global tech giants. Moreover, distributing computation in this way&nbsp;<b>could be kinder to the environment</b>, as&nbsp;smaller, geographically dispersed clusters&nbsp;may&nbsp;<b>reduce energy usage and heat density</b>&nbsp;compared to&nbsp;massive&nbsp;facilities. </p></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:left;"> In short, distributing AI workloads isn’t just about technical innovation—it offers a path towards a&nbsp;<b>more cost-effective, democratic, and sustainable future for AI as a whole</b>. </p></div><div style="padding:30px;"><div style="margin: 0 auto; border-top: 3px solid #91d3c9; width:50%;"></div></div><style>
  p span[style*="font-size"] { line-height: 1.6; }
</style><div style="padding-bottom:12px;padding-left:15px;padding-right:15px;padding-top:12px;"><p style="color:#222222;color:var(--wt-text-on-background-color) !important;font-family:&#39;Poppins&#39;,Helvetica,sans-serif;font-size:16px;line-height:24px;text-align:left;"><span style="color:rgb(0, 0, 0);font-size:medium;"><b>What do you think?</b></span><span style="color:rgb(0, 0, 0);font-size:medium;"> Is the future of AI in ever-larger clusters, or are we on the cusp of a more decentralised era? We’d love to hear your take—reply with your thoughts, share your perspective, or send this to someone who should be thinking about the future of AI.</span></p></div></div></div></div></div></div></div></div></div></div></div><div class="px-4"></div><div id="comments" class="px-4 space-y-8"><div class="pb-8"><div class="mx-auto max-w-2xl" style="padding-left: 15px; padding-right: 15px;"><div class="flex items-center justify-between gap-4 py-2"><h4 class="leading-none text-wt-text-on-background text-lg sm:text-xl font-bold">Reply</h4><div class=""><div class="relative inline-block text-left" data-headlessui-state=""><div><button class="inline-flex w-full justify-center whitespace-nowrap rounded-wt border bg-wt-background px-4 py-1 text-sm font-medium text-wt-text-on-background" id="headlessui-menu-button-_r_1d_" type="button" aria-haspopup="menu" aria-expanded="false" data-headlessui-state="">Most popular<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" aria-hidden="true" data-slot="icon" class="-mr-1 ml-2 h-5 w-5 text-wt-text-on-background"><path fill-rule="evenodd" d="M5.22 8.22a.75.75 0 0 1 1.06 0L10 11.94l3.72-3.72a.75.75 0 1 1 1.06 1.06l-4.25 4.25a.75.75 0 0 1-1.06 0L5.22 9.28a.75.75 0 0 1 0-1.06Z" clip-rule="evenodd"></path></svg></button></div></div></div></div><div class="pb-4"><div class="relative"><div class="overflow-hidden rounded-wt border border-gray-300 shadow-sm focus-within:border-wt-primary focus-within:ring-1 focus-within:ring-wt-primary"><label for="comment" class="sr-only">Add your comment</label><textarea rows="2" name="content" id="comment" class="block w-full resize-none border-0 bg-transparent py-3 text-sm focus:ring-0" placeholder="Add your comment..." required=""></textarea><div class="py-2" aria-hidden="true"><div class="py-px"><div class="h-6"></div></div></div></div><div class="absolute inset-x-0 bottom-0 flex justify-between py-2 pl-3 pr-2"><div class="flex items-center space-x-5"><div class="flex-shrink-0"></div></div><div class="flex-shrink-0 pr-1"><button type="button" class="border font-medium shadow-sm wt-button-font inline-flex items-center disabled:bg-gray-400 disabled:cursor-not-allowed focus:outline-none focus:ring-2 focus:ring-offset-2 justify-center transition-colors rounded-wt px-2 py-1 text-sm border-wt-primary text-wt-text-on-primary bg-wt-primary disabled:border-gray-500 focus:ring-wt-primary">Login</button></div></div></div><span class="opacity-75 wt-body-font text-wt-text-on-background text-xs sm:text-sm font-regular"><button type="button"><span class="underline">Login</span></button> or <button type="button"><span class="underline">Subscribe</span></button> to participate.</span></div></div></div><div class="mx-auto w-full max-w-2xl pb-8" style="padding-left: 15px; padding-right: 15px;"><h4 class="pb-6 leading-none sm:pb-2 text-wt-text-on-background text-lg sm:text-xl font-bold">Keep reading</h4><div class="space-y-6"><div class="flex flex-col"><a class="group relative flex rounded-wt grid w-full grid-cols-1 transition-all hover:bg-opacity-25 sm:my-6 sm:grid-cols-2 flex-row mb-6 gap-y-0 sm:mb-0 hover:bg-slate-100/50" data-discover="true" href="https://threads.attercop.com/p/why-does-gartner-consider-graphs-a-critical-enabler-right-now"><div class="z-10 col-span-1 w-full overflow-hidden rounded-wt border bg-slate-100/50"><figure class="aspect-social relative h-full overflow-hidden w-full"><img loading="lazy" width="800" height="421" alt="Why Does Gartner Consider Graphs a &#39;Critical Enabler&#39; Right Now? " class="absolute inset-0 h-full w-full object-cover" src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/Untitled_design.png"></figure></div><div class="z-10 col-span-1 p-4"><h2 class="line-clamp-2 text-wt-text-on-background wt-header-font text-wt-text-on-background text-md font-bold">Why Does Gartner Consider Graphs a 'Critical Enabler' Right Now? </h2><p class="font-regular mb-2 text-wt-text-on-background opacity-75 wt-header-font line-clamp-4 text-wt-text-on-background text-sm font-regular">In this issue of AI Threads, we examine how knowledge graphs complement large language models - and why combining structure with reasoning could be essential to the future of intelligent systems.</p><p class="mb-4 text-wt-text-on-background no-underline opacity-75 wt-header-font text-wt-text-on-background text-xs sm:text-sm font-regular"><span>Attercop Team</span><span> / </span><time datetime="2025-03-27T09:43:51.494Z"></time></p></div></a><a class="group relative flex rounded-wt grid w-full grid-cols-1 transition-all hover:bg-opacity-25 sm:my-6 sm:grid-cols-2 flex-row mb-6 gap-y-0 sm:mb-0 hover:bg-slate-100/50" data-discover="true" href="https://threads.attercop.com/p/introducing-ai-threads"><div class="z-10 col-span-1 w-full overflow-hidden rounded-wt border bg-slate-100/50"><figure class="aspect-social relative h-full overflow-hidden w-full"><img loading="lazy" width="800" height="421" alt="From Strategy to Scale: AI That Works" class="absolute inset-0 h-full w-full object-cover" src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/Introductory_Post_Thumbnail.png"></figure></div><div class="z-10 col-span-1 p-4"><h2 class="line-clamp-2 text-wt-text-on-background wt-header-font text-wt-text-on-background text-md font-bold">From Strategy to Scale: AI That Works</h2><p class="font-regular mb-2 text-wt-text-on-background opacity-75 wt-header-font line-clamp-4 text-wt-text-on-background text-sm font-regular">Practical insights and real-world strategies to help you unlock AI's full potential.</p><p class="mb-4 text-wt-text-on-background no-underline opacity-75 wt-header-font text-wt-text-on-background text-xs sm:text-sm font-regular"><span>Attercop Team</span><span> / </span><time datetime="2025-02-10T08:18:03.132Z"></time></p></div></a><a class="group relative flex rounded-wt grid w-full grid-cols-1 transition-all hover:bg-opacity-25 sm:my-6 sm:grid-cols-2 flex-row mb-6 gap-y-0 sm:mb-0 hover:bg-slate-100/50" data-discover="true" href="https://threads.attercop.com/p/why-define-your-ai-strategy"><div class="z-10 col-span-1 w-full overflow-hidden rounded-wt border bg-slate-100/50"><figure class="aspect-social relative h-full overflow-hidden w-full"><img loading="lazy" width="800" height="421" alt="Why define your AI strategy?" class="absolute inset-0 h-full w-full object-cover" src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/thumbnail.png"></figure></div><div class="z-10 col-span-1 p-4"><h2 class="line-clamp-2 text-wt-text-on-background wt-header-font text-wt-text-on-background text-md font-bold">Why define your AI strategy?</h2><p class="font-regular mb-2 text-wt-text-on-background opacity-75 wt-header-font line-clamp-4 text-wt-text-on-background text-sm font-regular">How strategy, governance, and a clear roadmap turn AI risk into opportunity.</p><p class="mb-4 text-wt-text-on-background no-underline opacity-75 wt-header-font text-wt-text-on-background text-xs sm:text-sm font-regular"><span>Graeme Cox | CEO, Attercop</span><span> / </span><time datetime="2025-09-03T14:00:00.000Z"></time></p></div></a></div><button type="button" class="hover flex items-center text-sm text-wt-primary transition-all hover:font-medium"><span>View more</span><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="ml-0.5 h-3 w-3 pt-0.5"><path stroke-linecap="round" stroke-linejoin="round" d="m8.25 4.5 7.5 7.5-7.5 7.5"></path></svg></button></div></div></div></div></main><footer class="px-4 sm:px-6 py-8" style="background-color:#0B4251"><div class="mx-auto w-full max-w-6xl"><div class="flex flex-col gap-y-6"><div class="grid grid-cols-1 gap-y-6 sm:grid-cols-3"><div class="sm:px-6"><div class="flex w-full flex-col items-center sm:items-start"><div class="flex items-center gap-x-2 py-4"><img src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/thumb_A_Beehiiv.png" alt="AI Threads spins a web of insight, tracing the connections between cutting-edge AI models, techniques and practical advice, scaling strategies, and real-world case studies to help businesses navigate the evolving AI landscape." width="40" height="40" class="overflow-hidden"><p style="color:#FFFFFF;font-family:Figtree, sans-ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto,&quot;Helvetica Neue&quot;, Arial, &quot;Noto Sans&quot;, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;" class="text-md font-semibold font-ariel">AI Threads</p></div><div class="text-center sm:text-left"><p style="color:#FFFFFF;font-family:Figtree, sans-ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto,&quot;Helvetica Neue&quot;, Arial, &quot;Noto Sans&quot;, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;" class="text-sm font-regular font-ariel">AI Threads spins a web of insight, tracing the connections between cutting-edge AI models, techniques and practical advice, scaling strategies, and real-world case studies to help businesses navigate the evolving AI landscape.</p></div></div></div><div class="mx-auto hidden sm:flex sm:flex-row w-1/2"><div class="flex grow basis-0 flex-col gap-y-3"><p style="color:#FFFFFF;font-family:Poppins, sans-ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto,&quot;Helvetica Neue&quot;, Arial, &quot;Noto Sans&quot;, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;" class="text-xs sm:text-sm font-semibold font-ariel">Home</p><div class="flex flex-col gap-y-2"><a class="sm:max-w-36 sm:text-wrap" data-discover="true" href="https://threads.attercop.com/"><p style="color:#FFFFFF;font-family:Figtree, sans-ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto,&quot;Helvetica Neue&quot;, Arial, &quot;Noto Sans&quot;, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;, &quot;Noto Color Emoji&quot;" class="text-xs sm:text-sm font-regular font-ariel">Posts</p></a></div></div></div><div class="flex flex-col items-center gap-y-4 sm:items-start sm:px-6"><div class="flex flex-col items-center w-full"><form class="w-full bg-transparent group rounded-wt" action="https://threads.attercop.com/create" method="post"><input hidden="" value="" name="ref"><input hidden="" value="" name="bhba"><input hidden="" value="a5018907-7b48-4354-bdfd-849bd48e337c" name="visit_token"><input type="hidden" name="redirect_path" value="/"><input type="hidden" name="sent_from_orchid" value="true"><input type="hidden" name="fallback_path" value="/"><input type="hidden" name="double_opt" value="true"><input type="hidden" name="trigger_redirect" value="false"><input hidden="" name="subscribe_error_message" value="Oops, something went wrong."><input hidden="" name="subscribe_success_message" value="Please check your email to confirm your subscription."><div class="flex flex-col"><div style="background-color:#F9FAFB;border:2px solid #91D3C9" class="flex w-full flex-col items-center sm:flex-row overflow-hidden p-1 rounded-md"><div class="flex w-full items-center" style="background-color:#F9FAFB"><input type="email" autocomplete="email" required="" style="font-family:Figtree;color:#111827" class="wt-button-font z-10 w-full border-none bg-transparent placeholder-shown:text-ellipsis text-lg focus:text-lg active:text-lg sm:text-lg" placeholder="Email" name="email"></div><input type="submit" style="background-color:#0B4251;color:#FFFFFF;font-family:Poppins" class="cursor-pointer px-5 py-3 font-semibold w-full sm:w-auto text-lg focus:text-lg active:text-lg sm:text-lg rounded-sm" value="Subscribe"></div><div class="flex flex-row mt-3 space-x-2"><input type="checkbox" class="w-4 h-4 border-gray-300 rounded text-lp-main-signup-cta-color focus:ring-indigo-500"><p class="text-xs font-medium" style="color:#000000">I consent to receive newsletters via email.<!-- --> <a href="https://www.beehiiv.com/tou" target="_blank" class="underline underline-offset-1 hover:opacity-80" rel="noreferrer">Terms of use</a> <!-- -->and<!-- --> <a href="https://www.beehiiv.com/privacy" target="_blank" class="underline underline-offset-1 hover:opacity-80" rel="noreferrer">Privacy policy</a>.</p></div></div></form></div><div class="flex flex-wrap items-center justify-center gap-2 sm:items-start sm:justify-start"><a target="_blank" rel="noreferrer" href="http://linkedin.com/company/attercop/" aria-label="LinkedIn" class="relative p-2"><div class="absolute left-0 top-0 h-full w-full rounded-full bg-black opacity-10"></div><div class="rounded-full bg-black p-2 text-white" style="background-color:#FFFFFF;color:#0B4251"><svg class="h-3 w-3" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" fill="currentColor"><path d="M100.28 448H7.4V148.9h92.88zM53.79 108.1C24.09 108.1 0 83.5 0 53.8a53.79 53.79 0 0 1 107.58 0c0 29.7-24.1 54.3-53.79 54.3zM447.9 448h-92.68V302.4c0-34.7-.7-79.2-48.29-79.2-48.29 0-55.69 37.7-55.69 76.7V448h-92.78V148.9h89.08v40.8h1.3c12.4-23.5 42.69-48.3 87.88-48.3 94 0 111.28 61.9 111.28 142.3V448z"></path></svg></div></a><a target="_blank" rel="noreferrer" href="https://rss.beehiiv.com/feeds/XTJ6gNWfGC.xml" aria-label="RSS" class="relative p-2"><div class="absolute left-0 top-0 h-full w-full rounded-full bg-black opacity-10"></div><div class="rounded-full bg-black p-2 text-white" style="background-color:#FFFFFF;color:#0B4251"><svg xmlns="http://www.w3.org/2000/svg" role="img" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="h-3 w-3" fill="currentColor"><path d="M19.199 24C19.199 13.467 10.533 4.8 0 4.8V0c13.165 0 24 10.835 24 24h-4.801zM3.291 17.415c1.814 0 3.293 1.479 3.293 3.295 0 1.813-1.485 3.29-3.301 3.29C1.47 24 0 22.526 0 20.71s1.475-3.294 3.291-3.295zM15.909 24h-4.665c0-6.169-5.075-11.245-11.244-11.245V8.09c8.727 0 15.909 7.184 15.909 15.91z"></path></svg></div></a></div></div></div><div class="flex w-full flex-col items-center gap-y-4 sm:flex-row sm:justify-between sm:px-6"><div class="w-full text-center sm:w-1/2 sm:text-left"><p style="color:#FFFFFF" class="!text-xs text-md font-light font-ariel">© 2025 Attercop AI Threads.</p></div><div class="flex w-full flex-col items-center gap-y-4 sm:w-1/2 sm:flex-row sm:justify-end sm:gap-x-4"><div class="flex gap-x-2 sm:gap-x-5"><a href="https://beehiiv.com/privacy" target="_blank" class="whitespace-nowrap underline underline-offset-1 hover:opacity-80" rel="noreferrer" style="color:#FFFFFF"><p style="color:#FFFFFF" class="!text-xs text-md font-light font-ariel">Privacy policy</p></a><a href="https://beehiiv.com/tou" target="_blank" class="whitespace-nowrap underline underline-offset-1 hover:opacity-80" rel="noreferrer" style="color:#FFFFFF"><p style="color:#FFFFFF" class="!text-xs text-md font-light font-ariel">Terms of use</p></a></div><div class="w-fit"><span class="text-gray-700 text-md font-regular"><a target="_blank" rel="noreferrer" href="https://www.beehiiv.com/?utm_source=AI%20Threads&amp;utm_medium=footer" class="extra-light flex items-center rounded border border-gray-300 bg-white px-3 py-2 text-xs"><svg xmlns="http://www.w3.org/2000/svg" fill="none" class="mr-2 h-4 w-4" viewBox="0 0 30 30"><path fill="#0B0D2A" d="M25.692 13.168H3.866c-.556 0-1.01-.458-1.01-1.017V10.6c0-1.755 1.414-3.178 3.157-3.178H23.52c1.743 0 3.157 1.423 3.157 3.177v1.55a.98.98 0 0 1-.985 1.018ZM21.092 5.745H8.462a1.044 1.044 0 0 1-1.036-1.042C7.426 2.11 9.523 0 12.099 0h5.356c2.576 0 4.673 2.11 4.673 4.703 0 .584-.455 1.042-1.036 1.042ZM26.702 14.845H2.855C1.288 14.845 0 16.142 0 17.718c0 1.576 1.288 2.872 2.855 2.872h8.462c.303-1.627 1.718-2.872 3.436-2.872a3.532 3.532 0 0 1 3.46 2.872h8.463c1.567 0 2.855-1.296 2.855-2.872 0-1.576-1.263-2.873-2.83-2.873ZM25.663 22.268h-7.401v5.745h5.557c1.567 0 2.855-1.297 2.855-2.873v-1.83a1.006 1.006 0 0 0-1.01-1.042ZM3.864 22.268c-.556 0-1.036.457-1.036 1.042v1.83c0 1.576 1.288 2.873 2.855 2.873h5.557v-5.745H3.864Z"></path></svg><span class="text-gray-700 text-md font-regular">Powered by beehiiv</span></a></span></div></div></div></div></div></footer></div></div><script>((d,u)=>{if(!window.history.state||!window.history.state.key){let h=Math.random().toString(32).slice(2);window.history.replaceState({key:h},"")}try{let g=JSON.parse(sessionStorage.getItem(d)||"{}")[u||window.history.state.key];typeof g=="number"&&window.scrollTo(0,g)}catch(h){console.error(h),sessionStorage.removeItem(d)}})("positions", null)</script><script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'9a28693499dffc58',t:'MTc2MzgxNDM0Mi4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script><iframe height="1" width="1" style="position: absolute; top: 0px; left: 0px; border: none; visibility: hidden;" src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/saved_resource.html"></iframe><script src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/q2je1lwj" async="" id="birdie-snippet"></script><iframe scrolling="no" frameborder="0" allowtransparency="true" src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/widget_iframe.2f70fb173b9000da126c79afe2098f02.html" title="Twitter settings iframe" style="display: none;"></iframe><iframe id="rufous-sandbox" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" style="position: absolute; visibility: hidden; display: none; width: 0px; height: 0px; padding: 0px; border: none;" title="Twitter analytics iframe" src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/saved_resource(1).html"></iframe><script src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/widgets.js" data-twitter-embed="true"></script><script src="./The Future of AI Training_ Scaling Smarter, Not Bigger_files/embed(1).js" data-bluesky-embed="true"></script></body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>